{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use BagofWords and Word2Vec to do a spam email classification\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "import os\n",
    "\n",
    "USE_KAGGLE = False\n",
    "if USE_KAGGLE:\n",
    "    !pip install kaggle\n",
    "\n",
    "    download_dir = \"data\"\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "    # requires kaggle.json in ~/.kaggle\n",
    "    os.system(f'kaggle datasets download -d purusinghvi/email-spam-classification-dataset -p {download_dir} --unzip')\n",
    "\n",
    "    csv_path = os.path.join(download_dir, \"combined_data.csv\")\n",
    "    if os.path.exists(csv_path):\n",
    "        print(f\"Downloaded and extracted: {csv_path}\")\n",
    "    else:\n",
    "        print(\"Download failed or file not found.\")\n",
    "\n",
    "else:\n",
    "    extract_folder = \"data\"\n",
    "\n",
    "    # Ensure extraction folder exists\n",
    "    os.makedirs(extract_folder, exist_ok=True)\n",
    "\n",
    "    # Define file paths\n",
    "    zip_path = \"data/email-spam-classification-dataset.zip\"\n",
    "    extract_folder = \"data\"\n",
    "\n",
    "    # Check if the dataset is already downloaded\n",
    "    if not os.path.exists(zip_path):\n",
    "        print(\"Downloading dataset...\")\n",
    "        !curl -L -o \"{extract_folder}/email-spam-classification-dataset.zip\" https://www.kaggle.com/api/v1/datasets/download/purusinghvi/email-spam-classification-dataset\n",
    "    else:\n",
    "        print(\"Dataset already downloaded.\")\n",
    "\n",
    "    # Check if the dataset is already extracted\n",
    "    if not os.path.exists(os.path.join(extract_folder, \"email.csv\")):\n",
    "        print(\"Extracting dataset...\")\n",
    "        !unzip -o {zip_path} -d {extract_folder}\n",
    "    else:\n",
    "        print(\"Dataset already extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/combined_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print total number of emails, non-spam emails, and spam emails\n",
    "print(\"Total number of emails:\", len(df))\n",
    "print(\"Number of non-spam emails:\", len(df[df[\"label\"] == 0]))\n",
    "print(\"Number of spam emails:\", len(df[df[\"label\"] == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"text\"].values\n",
    "y = df[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=87)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train_bow = count_vectorizer.fit_transform(X_train)\n",
    "X_test_bow = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer = CountVectorizer(max_features=1000)\n",
    "X_train_bow = count_vectorizer.fit_transform(X_train)\n",
    "X_test_bow = count_vectorizer.transform(X_test)\n",
    "\n",
    "print(\"shape of X_train_bow:\", X_train_bow.shape)\n",
    "print(\"shape of X_test_bow:\", X_test_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_train_bow.toarray()[:100,:200])\n",
    "plt.xlabel(\"Word\")\n",
    "plt.ylabel(\"Email\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "lr_bow = LogisticRegression(max_iter=len(y_train))\n",
    "lr_bow.fit(X_train_bow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_bow = lr_bow.predict(X_train_bow)\n",
    "print(f\"Accuracy with BoW: {accuracy_score(y_train, y_pred_train_bow)}\")\n",
    "\n",
    "y_pred_test_bow = lr_bow.predict(X_test_bow)\n",
    "print(f\"Accuracy with BoW: {accuracy_score(y_test, y_pred_test_bow)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gensim to load pre-trained Word2Vec model\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "wv = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embedding of a word\n",
    "print(wv[\"computer\"].shape)\n",
    "plt.plot(wv[\"computer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show that man - woman = king - queen\n",
    "# queen = king + woman - man\n",
    "# w_1, w_2, w_3 = 'man', 'woman', 'king'\n",
    "# similarily, apple - banana = red - ?\n",
    "w_1, w_2, w_3 = 'apple', 'banana', 'red'\n",
    "\n",
    "man, woman, king = wv[w_1], wv[w_2], wv[w_3]\n",
    "tmp = king + woman - man\n",
    "\n",
    "# find the most similar word to tmp\n",
    "wv.most_similar(positive=[tmp])\n",
    "\n",
    "# Try other calculations by yourself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Given one word, find the most similar email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5000 # number of emails to use\n",
    "import textwrap\n",
    "\n",
    "# get embeddings of all emails\n",
    "import re\n",
    "X_w2v = np.zeros((N, wv.vector_size))\n",
    "for i in tqdm.tqdm(range(N)):\n",
    "    email = X[i]\n",
    "    # convert to lowercase\n",
    "    email = email.lower()\n",
    "    # use regex to split the email into words and remove non-alphabetic characters and \"_\" character\n",
    "    words = re.findall(r\"[a-zA-Z]+\", email)\n",
    "    for word in words:\n",
    "        if word in wv:\n",
    "            X_w2v[i] += wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize X_w2v\n",
    "X_w2v_normalized = X_w2v / np.linalg.norm(X_w2v, axis=1).reshape(-1, 1)\n",
    "# fill nan with 0\n",
    "X_w2v_normalized = np.nan_to_num(X_w2v_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use BoW to find the most similar email\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_bow = count_vectorizer.fit_transform(X[:N]).toarray()\n",
    "X_bow_normalized = X_bow / np.linalg.norm(X_bow, axis=1).reshape(-1, 1)\n",
    "X_bow_normalized = np.nan_to_num(X_bow_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given one topic, find the most similar email\n",
    "topic = \"romantic\"\n",
    "topic_vec = wv[topic]\n",
    "topic_vec_normalized = topic_vec / np.linalg.norm(topic_vec)\n",
    "similarity = np.dot(X_w2v_normalized, topic_vec_normalized)\n",
    "most_similar_email = X[np.argmax(similarity)]\n",
    "print(\"Found by Word2Vec:\")\n",
    "print(textwrap.fill(most_similar_email, 100))\n",
    "print(\"Number of times the topic word appears in the email:\", most_similar_email.lower().count(topic))\n",
    "\n",
    "\n",
    "# given one topic, find the most similar email\n",
    "topic_vec = count_vectorizer.transform([topic]).toarray()\n",
    "topic_vec_normalized = topic_vec / np.linalg.norm(topic_vec)\n",
    "similarity = np.dot(X_bow_normalized, topic_vec_normalized.T).flatten()\n",
    "most_similar_email = X[np.argmax(similarity)]\n",
    "print(\"\\nFound by BoW:\")\n",
    "print(textwrap.fill(most_similar_email, 100))\n",
    "print(\"Number of times the topic word appears in the email:\", most_similar_email.lower().count(topic))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4om",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
